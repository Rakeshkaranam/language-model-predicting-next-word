{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lets Predict the Next Word using LSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoWvs0xNkfUa"
      },
      "source": [
        "## This notebook contains the code to implement next word prediction model using RNNs(LSTM) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdqb4usPkMhb"
      },
      "source": [
        "### First we will mount to the Google Drive to read the data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HS7kU59Hm3U",
        "outputId": "7d1b52b6-c1ab-4af4-b3b7-03b34f40fbe3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FudQ7nPTHn85"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import Concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm.notebook import tqdm_notebook"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70Yn6v81kz-X"
      },
      "source": [
        "### I have used Harry Potter Novels as the corpus, but you are free to choose any text corpus\n",
        "\n",
        "#### Due to memory contraints, I trained it only on one part by setting max_files to 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEPsenOzIUrT",
        "outputId": "ec65246b-d12f-4b50-d4a4-b9ceb832ad17"
      },
      "source": [
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r',encoding=\"UTF-8\") as f:\n",
        "        return f.read()\n",
        "    \n",
        "training_text = \"\"\n",
        "# iterate through all files\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/NextWordPrediction/Data/\"\n",
        "max_file = 1\n",
        "count = 0\n",
        "for file in os.listdir(path):\n",
        "    count+=1\n",
        "    if count>max_file:\n",
        "        break\n",
        "    print(file)\n",
        "    # Check whether file is in text format or not\n",
        "    if file.endswith(\".txt\"):\n",
        "        file_path = path+file\n",
        "        training_text+=read_text_file(file_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Book 3 - The Prisoner of Azkaban.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bfuCYydI6w-",
        "outputId": "f85fa3b4-6158-40f7-b1a8-58f0b6dbaf8e"
      },
      "source": [
        "len(training_text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "676978"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoIwDw43lapo"
      },
      "source": [
        "#### Remove unwanted junk characters and tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tYrV2pLJFFO"
      },
      "source": [
        "cleaned = re.sub(r'\\W+', ' ', training_text).lower()\n",
        "tokens = word_tokenize(cleaned)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nswh6yYlidR"
      },
      "source": [
        "#### Here, you need to specify the window length to divide sequences for training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGvnP6kbJNz2"
      },
      "source": [
        "train_len = 4\n",
        "text_sequences = []\n",
        "for i in range(train_len,len(tokens)):\n",
        "    seq = tokens[i-train_len:i]\n",
        "    text_sequences.append(seq)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK1kArAolxNW"
      },
      "source": [
        "#### You can see below how the sequences are formed with a window lenght of 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU3B3C32JRyH",
        "outputId": "2b227c9d-8801-406d-b2c0-08623600bc65"
      },
      "source": [
        "text_sequences[0:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['owl', 'post', 'harry', 'potter'],\n",
              " ['post', 'harry', 'potter', 'was'],\n",
              " ['harry', 'potter', 'was', 'a'],\n",
              " ['potter', 'was', 'a', 'highly'],\n",
              " ['was', 'a', 'highly', 'unusual'],\n",
              " ['a', 'highly', 'unusual', 'boy'],\n",
              " ['highly', 'unusual', 'boy', 'in'],\n",
              " ['unusual', 'boy', 'in', 'many'],\n",
              " ['boy', 'in', 'many', 'ways'],\n",
              " ['in', 'many', 'ways', 'for']]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hmVT16mDFP"
      },
      "source": [
        "#### Now, you need to tokenize the data to convert the above tokens into unique indexes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQPf3gtNJVNU",
        "outputId": "0d71751b-e436-480b-d055-678ea3237574"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "train_sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "print('Found %s unique tokens.' % len(tokenizer.word_counts))\n",
        "vocab =  len(tokenizer.word_counts)+1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7895 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxRRukYFJX8p"
      },
      "source": [
        "n_sequences = np.empty([len(train_sequences),train_len], dtype='int32')\n",
        "for i in range(len(train_sequences)):\n",
        "    n_sequences[i] = train_sequences[i]\n",
        "\n",
        "x = n_sequences[:,:-1]\n",
        "y = n_sequences[:,-1]\n",
        "#y_cat = to_categorical(y, num_classes=vocab)\n",
        "#seq_len = train_inputs.shape[1]\n",
        "#train_inputs.shape"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdfmfx7UJi4d"
      },
      "source": [
        "y = to_categorical(y,num_classes=vocab)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5IZpmRjmRRP"
      },
      "source": [
        "#### Now, it is time to build the super cool Next Word Prediction Model\n",
        "\n",
        "##### Model Architecure is configurable and you are free to try and figure out different structures and hyper parameters\n",
        "\n",
        "##### Below architecure is just one way to train the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amJS84aiJkdd",
        "outputId": "57d537e2-3902-4f00-bc9f-a66f7df8ba07"
      },
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "#model = load_model(\"mymodel.h5\")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab,10, input_length=3))\n",
        "model.add(LSTM(50,return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(50,activation='relu'))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "# compile network\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#model.fit(x,y_cat,epochs=,verbose=1)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 3, 10)             78960     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 3, 50)             12200     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 7896)              402696    \n",
            "=================================================================\n",
            "Total params: 516,606\n",
            "Trainable params: 516,606\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "movC1p4VKMcP",
        "outputId": "5ba98849-35ad-4215-a38d-def9ad28243c"
      },
      "source": [
        "history = model.fit(x, y,validation_split=0.05,batch_size=128, epochs=50)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "871/871 [==============================] - 26s 19ms/step - loss: 6.7670 - accuracy: 0.0462 - val_loss: 6.4687 - val_accuracy: 0.0459\n",
            "Epoch 2/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 6.3637 - accuracy: 0.0539 - val_loss: 6.1788 - val_accuracy: 0.0711\n",
            "Epoch 3/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 6.0225 - accuracy: 0.0896 - val_loss: 5.9245 - val_accuracy: 0.1040\n",
            "Epoch 4/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 5.7334 - accuracy: 0.1174 - val_loss: 5.7409 - val_accuracy: 0.1196\n",
            "Epoch 5/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 5.5188 - accuracy: 0.1340 - val_loss: 5.6835 - val_accuracy: 0.1259\n",
            "Epoch 6/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 5.3674 - accuracy: 0.1423 - val_loss: 5.6638 - val_accuracy: 0.1353\n",
            "Epoch 7/50\n",
            "871/871 [==============================] - 15s 17ms/step - loss: 5.2494 - accuracy: 0.1482 - val_loss: 5.6512 - val_accuracy: 0.1409\n",
            "Epoch 8/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 5.1438 - accuracy: 0.1540 - val_loss: 5.6738 - val_accuracy: 0.1462\n",
            "Epoch 9/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 5.0490 - accuracy: 0.1589 - val_loss: 5.6754 - val_accuracy: 0.1475\n",
            "Epoch 10/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 4.9626 - accuracy: 0.1628 - val_loss: 5.7245 - val_accuracy: 0.1484\n",
            "Epoch 11/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 4.8820 - accuracy: 0.1668 - val_loss: 5.7645 - val_accuracy: 0.1533\n",
            "Epoch 12/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 4.8049 - accuracy: 0.1714 - val_loss: 5.7940 - val_accuracy: 0.1545\n",
            "Epoch 13/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 4.7312 - accuracy: 0.1743 - val_loss: 5.8893 - val_accuracy: 0.1571\n",
            "Epoch 14/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 4.6580 - accuracy: 0.1783 - val_loss: 5.9681 - val_accuracy: 0.1571\n",
            "Epoch 15/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 4.5876 - accuracy: 0.1814 - val_loss: 6.0490 - val_accuracy: 0.1537\n",
            "Epoch 16/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 4.5175 - accuracy: 0.1851 - val_loss: 6.1047 - val_accuracy: 0.1569\n",
            "Epoch 17/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 4.4511 - accuracy: 0.1896 - val_loss: 6.2371 - val_accuracy: 0.1590\n",
            "Epoch 18/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 4.3874 - accuracy: 0.1919 - val_loss: 6.3775 - val_accuracy: 0.1562\n",
            "Epoch 19/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 4.3279 - accuracy: 0.1962 - val_loss: 6.5139 - val_accuracy: 0.1600\n",
            "Epoch 20/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 4.2705 - accuracy: 0.1997 - val_loss: 6.6597 - val_accuracy: 0.1595\n",
            "Epoch 21/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 4.2174 - accuracy: 0.2039 - val_loss: 6.8158 - val_accuracy: 0.1619\n",
            "Epoch 22/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 4.1664 - accuracy: 0.2075 - val_loss: 6.9209 - val_accuracy: 0.1615\n",
            "Epoch 23/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 4.1208 - accuracy: 0.2113 - val_loss: 7.0994 - val_accuracy: 0.1581\n",
            "Epoch 24/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 4.0783 - accuracy: 0.2168 - val_loss: 7.2245 - val_accuracy: 0.1612\n",
            "Epoch 25/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 4.0367 - accuracy: 0.2207 - val_loss: 7.4296 - val_accuracy: 0.1590\n",
            "Epoch 26/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 4.0005 - accuracy: 0.2253 - val_loss: 7.4725 - val_accuracy: 0.1564\n",
            "Epoch 27/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.9653 - accuracy: 0.2292 - val_loss: 7.6171 - val_accuracy: 0.1600\n",
            "Epoch 28/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.9309 - accuracy: 0.2332 - val_loss: 7.8047 - val_accuracy: 0.1595\n",
            "Epoch 29/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.9008 - accuracy: 0.2356 - val_loss: 7.9616 - val_accuracy: 0.1586\n",
            "Epoch 30/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.8729 - accuracy: 0.2392 - val_loss: 8.0445 - val_accuracy: 0.1576\n",
            "Epoch 31/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.8451 - accuracy: 0.2416 - val_loss: 8.2260 - val_accuracy: 0.1583\n",
            "Epoch 32/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.8191 - accuracy: 0.2469 - val_loss: 8.3087 - val_accuracy: 0.1586\n",
            "Epoch 33/50\n",
            "871/871 [==============================] - 15s 17ms/step - loss: 3.7937 - accuracy: 0.2487 - val_loss: 8.4164 - val_accuracy: 0.1588\n",
            "Epoch 34/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.7687 - accuracy: 0.2527 - val_loss: 8.5492 - val_accuracy: 0.1595\n",
            "Epoch 35/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 3.7469 - accuracy: 0.2555 - val_loss: 8.5832 - val_accuracy: 0.1538\n",
            "Epoch 36/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 3.7262 - accuracy: 0.2580 - val_loss: 8.7636 - val_accuracy: 0.1583\n",
            "Epoch 37/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.7048 - accuracy: 0.2614 - val_loss: 8.8500 - val_accuracy: 0.1530\n",
            "Epoch 38/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.6871 - accuracy: 0.2633 - val_loss: 8.9276 - val_accuracy: 0.1588\n",
            "Epoch 39/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.6673 - accuracy: 0.2649 - val_loss: 9.0548 - val_accuracy: 0.1537\n",
            "Epoch 40/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.6481 - accuracy: 0.2697 - val_loss: 9.1432 - val_accuracy: 0.1552\n",
            "Epoch 41/50\n",
            "871/871 [==============================] - 15s 17ms/step - loss: 3.6305 - accuracy: 0.2707 - val_loss: 9.2081 - val_accuracy: 0.1542\n",
            "Epoch 42/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.6150 - accuracy: 0.2735 - val_loss: 9.3454 - val_accuracy: 0.1562\n",
            "Epoch 43/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 3.5996 - accuracy: 0.2750 - val_loss: 9.3549 - val_accuracy: 0.1532\n",
            "Epoch 44/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.5823 - accuracy: 0.2769 - val_loss: 9.5670 - val_accuracy: 0.1552\n",
            "Epoch 45/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.5683 - accuracy: 0.2801 - val_loss: 9.6266 - val_accuracy: 0.1586\n",
            "Epoch 46/50\n",
            "871/871 [==============================] - 14s 17ms/step - loss: 3.5527 - accuracy: 0.2817 - val_loss: 9.6308 - val_accuracy: 0.1552\n",
            "Epoch 47/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.5388 - accuracy: 0.2836 - val_loss: 9.7395 - val_accuracy: 0.1552\n",
            "Epoch 48/50\n",
            "871/871 [==============================] - 14s 16ms/step - loss: 3.5246 - accuracy: 0.2858 - val_loss: 9.8392 - val_accuracy: 0.1557\n",
            "Epoch 49/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 3.5108 - accuracy: 0.2868 - val_loss: 9.9519 - val_accuracy: 0.1537\n",
            "Epoch 50/50\n",
            "871/871 [==============================] - 13s 15ms/step - loss: 3.4992 - accuracy: 0.2887 - val_loss: 10.0472 - val_accuracy: 0.1590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFzwc8R8mySa"
      },
      "source": [
        "#### Okay, the wait is over, lets predict some words\n",
        "\n",
        "##### Predict is used to predict the next word\n",
        "\n",
        "##### Generate text is used to generate some random text by using the starting few words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZqjKZAPRMua"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def predict(model,text):\n",
        "  text = text.lower()\n",
        "  test_sequences = tokenizer.texts_to_sequences([text])\n",
        "  pad_encoded = pad_sequences(test_sequences, maxlen=3, truncating='pre')\n",
        "  predicted_word = model.predict(pad_encoded)\n",
        "  label = predicted_word.argmax(axis=1)\n",
        "  return tokenizer.index_word[label[0]]\n",
        "\n",
        "def generate_text(model, start_text, max_words =25):\n",
        "  output = start_text\n",
        "  for i in range(max_words):\n",
        "      output+=\" \"+predict(model,output)\n",
        "  return output\n",
        "  "
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AJnCvTbxNi7p",
        "outputId": "34a49574-d613-4900-efcd-83f57262ff1d"
      },
      "source": [
        "predict(model,\"Harry took the invisibility\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cloak'"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "W9zVPPkgh94a",
        "outputId": "29badc3e-4b00-4539-bddc-8282b50092af"
      },
      "source": [
        "generate_text(model, \"Harry took the invisibility\", max_words =25)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Harry took the invisibility cloak and the decision of the committee member from binder a tadpole and the next day and began to chop the room of the crowd'"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "TlqO8AKXjvwc",
        "outputId": "43ac7f93-c4b8-459d-ae6c-bd4e4f1eafdb"
      },
      "source": [
        "generate_text(model, \"Harry took the invisibility\", max_words =50)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Harry took the invisibility cloak and the decision of the committee member from binder a tadpole and the next day and began to chop the room of the crowd and lanterns hopping around at the moment he was recognizable at the back of the room unsmiling and watchful the castle and then propelled itself'"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNh7JDSWjxAY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}